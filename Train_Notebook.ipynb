{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Use GANs to create art - Monet CycleGan Project**<br>\n","**COL_ITMACHLI Group (Kaggle Competition)**\n","- Participated in Kaggle competitions: https://www.kaggle.com/code/chenshiri/monetcyclegan?scriptVersionId=168171237\n","- The model was created using the following resources:\n"," - Based on our on research.\n"," - The knowledge we gained from Dr Moshe Butman the facultiy.\n"," - Information from a known research paper named \"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\". The research was published by Berkeley AI Research (BAIR) laboratory, Link:<br> https://arxiv.org/pdf/1703.10593.pdf\n"," - Information from a research paper \"CycleGAN with Better Cycles\" by Tongzhou Wang and Yihan Lin, Link:<br>https://www.tongzhouwang.info/better_cycles/report.pdf\n"," - Information from a research paper \"Instance Normalization:\n","The Missing Ingredient for Fast Stylization\" by Dmitry Ulyanov, Andrea Vedald and Victor Lempitsky, Link:<br>https://www.tongzhouwang.info/better_cycles/report.pdf\n","- Attached **PDF Report** containing a full explanation of the model, the tests, and the improvements we made <br> during the learning process: https://drive.google.com/file/d/1ZKvcFjhPpJBqAtH_D9puP1ic-sjrkDa0/view?usp=sharing\n","- Written by:\n","  - Matan Ofri\n","  - Itamar Kirsch\n","  - Chen Shiri\n","  - Liel Pargamentnik\n","- The model was trained on RTX 4090 GPU.\n","\n"],"metadata":{"id":"GhMKO7YU3E80"}},{"cell_type":"markdown","source":["Tensorflow Addons\n","- This addon must be installed for the \"InstanceNormalization\" library function."],"metadata":{"id":"Rz5hpTnd2nWS"}},{"cell_type":"code","source":["!pip install tensorflow-addons"],"metadata":{"id":"TICGwG67zY2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vE7N_Uj4BAgc"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_addons as tfa # Install tensorflow-addons\n","import keras as ker\n","import numpy as np\n","import os\n","import cv2\n","import glob\n","from keras import backend as K\n","from keras import initializers, layers, Input, backend, Model, losses, optimizers\n","from keras.layers import Conv2D, Conv2DTranspose, ReLU, Add, Activation ,Concatenate, LeakyReLU\n","from keras.models import Sequential\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.optimizers import Adam\n","from keras.losses import BinaryCrossentropy\n","from keras.initializers import RandomNormal\n","from keras.metrics import BinaryAccuracy\n","from keras.utils import register_keras_serializable\n","from keras.callbacks import Callback\n","from matplotlib import pyplot as plt\n","from numpy import load ,ones ,zeros ,asarray\n","from numpy.random import randint\n","from google.colab.patches import cv2_imshow\n","from random import random\n","import random as rand\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# CycleGAN Model\n","\n","---"],"metadata":{"id":"TVoflt0BbwWh"}},{"cell_type":"markdown","source":["Hyperparameters"],"metadata":{"id":"OeeFSFX7PUcI"}},{"cell_type":"code","source":["img_height = 320\n","img_width = 320\n","img_channels = 3\n","batch_size = 4\n","epochs = 100\n","learning_rate=0.0002\n","beta_val=0.5\n","img_shape = (img_width, img_height, img_channels)\n","num_of_resnet_blocks = 12"],"metadata":{"id":"2_-_ZdYtEy8u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reflection Padding Layer\n","- Taken from Keras library: https://keras.io/examples/generative/cyclegan."],"metadata":{"id":"8xbzilNGO6DL"}},{"cell_type":"code","source":["@register_keras_serializable()\n","class ReflectionPadding2D(layers.Layer):\n","    def __init__(self, padding=(1, 1), **kwargs):\n","        self.padding = tuple(padding)\n","        super().__init__(**kwargs)\n","\n","    # Override in Layer\n","    def call(self, input_tensor, mask=None):\n","        padding_width, padding_height = self.padding\n","        padding_tensor = [\n","            [0, 0],\n","            [padding_height, padding_height],\n","            [padding_width, padding_width],\n","            [0, 0],\n","        ]\n","        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")"],"metadata":{"id":"ZuvOoHzxhxY6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Initializing the blocks for the generator\n","- The **Encoder Block** is designed to downsample the input layer, extracting and compressing features through convolution, and optionally applying instance normalization for consistent feature scaling, followed by a ReLU activation for non-linearity.\n","- The **ResNet Block** enhances feature representation without increasing the depth or complexity significantly by applying a series of convolutions within a residual learning framework, allowing the input to be directly added to the output, which helps mitigate the vanishing gradient problem in deep networks.\n","- The **Decoder Block** performs the inverse operation of the encoder, upsampling the input layer and reconstructing the image or feature map dimensions through transposed convolution, with optional instance normalization and ReLU activation to maintain feature consistency and non-linearity."],"metadata":{"id":"B-Y_2DvZkrJ7"}},{"cell_type":"code","source":["conv_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n","gamma_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n","\n","# Encoder Block\n","def encoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, activation=ker.layers.ReLU(), name='encoder'):\n","    enc_layer = ker.layers.Conv2D(filters, size,\n","                     strides=strides,\n","                     padding='same',\n","                     use_bias=False,\n","                     kernel_initializer=conv_initializer,\n","                     name=f'block-{name}')(input_layer)\n","\n","    if apply_instancenorm:\n","        enc_layer = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(enc_layer)\n","    enc_layer = activation(enc_layer)\n","    return enc_layer\n","\n","# Resnet Block\n","def resnet_block(input_layer, size=3, strides=1, name='resnet'):\n","    filters = input_layer.shape[-1]\n","    res_layer = ReflectionPadding2D()(input_layer)\n","    res_layer = ker.layers.Conv2D(filters, size,\n","                     strides=strides,\n","                     padding='valid',\n","                     use_bias=False,\n","                     kernel_initializer=conv_initializer,\n","                     name=f'block-{name}_1')(res_layer)\n","    res_layer = ker.layers.ReLU()(res_layer)\n","    res_layer = ReflectionPadding2D()(res_layer)\n","    res_layer = ker.layers.Conv2D(filters, size,\n","                     strides=strides,\n","                     padding='valid',\n","                     use_bias=False,\n","                     kernel_initializer=conv_initializer,\n","                     name=f'block-{name}_2')(res_layer)\n","    res_layer = ker.layers.Add()([res_layer, input_layer])\n","    return res_layer\n","\n","# Decoder Block\n","def decoder_block(input_layer, filters, size=3, strides=2, apply_instancenorm=True, name='decoder'):\n","    dec_layer = ker.layers.Conv2DTranspose(filters, size,\n","                              strides=strides,\n","                              padding='same',\n","                              use_bias=False,\n","                              kernel_initializer=conv_initializer,\n","                              name=f'block-{name}')(input_layer)\n","\n","    if apply_instancenorm:\n","        dec_layer = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(dec_layer)\n","    dec_layer = ker.layers.ReLU()(dec_layer)\n","    return dec_layer"],"metadata":{"id":"hcCul_SOTPFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generator Model"],"metadata":{"id":"emRelwk4JPAc"}},{"cell_type":"code","source":["def generator(height=img_height, width=img_width, channels=img_channels, resnet_blocks=num_of_resnet_blocks):\n","    # Input:\n","    inputs = ker.layers.Input(shape=[height, width, channels], name='input_image')\n","\n","    # Encoder:\n","    # 1 - (batch_size, 256, 256, 64)\n","    enc_1 = encoder_block(inputs, 64,  7, 1, apply_instancenorm=False, activation=ker.layers.ReLU(), name='enc_1')\n","    # 2 - (batch_size, 128, 128, 128)\n","    enc_2 = encoder_block(enc_1, 128, 3, 2, apply_instancenorm=True, activation=ker.layers.ReLU(), name='enc_2')\n","    # 3 - (batch_size, 64, 64, 256)\n","    enc_3 = encoder_block(enc_2, 256, 3, 2, apply_instancenorm=True, activation=ker.layers.ReLU(), name='enc_3')\n","\n","    # Resnet:\n","    # The value of \"resnet_blocks\" times, every Layer is (batch_size, 64, 64, 256)\n","    res = enc_3\n","    for i in range(resnet_blocks):\n","        res = resnet_block(res, 3, 1, name=f'resnet_block_{i+1}')\n","\n","    # Decoder:\n","    skip_con = ker.layers.Concatenate(name='enc_dec_skip_1')([res, enc_3])\n","    # 1 - (batch_size, 128, 128, 128)\n","    dec_1 = decoder_block(skip_con, 128, 3, 2, apply_instancenorm=True, name='block_1')\n","    skip_con = ker.layers.Concatenate(name='enc_dec_skip_2')([dec_1, enc_2])\n","    # 2 - (batch_size, 256, 256, 64)\n","    dec_2 = decoder_block(skip_con, 64,  3, 2, apply_instancenorm=True, name='block_2')\n","    skip_con = ker.layers.Concatenate(name='enc_dec_skip_3')([dec_2, enc_1])\n","\n","    # Output:\n","    # 1 - (batch_size, 256, 256, 3)\n","    outputs = last = ker.layers.Conv2D(channels, 7,\n","                              strides=1, padding='same',\n","                              kernel_initializer=conv_initializer,\n","                              use_bias=False,\n","                              activation='tanh',\n","                              name='decoder_output_block')(skip_con)\n","\n","    generator = Model(inputs, outputs)\n","    return generator"],"metadata":{"id":"8GwQCyCxTM2F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Discriminator Model"],"metadata":{"id":"DXXssKpoJIBB"}},{"cell_type":"code","source":["def discriminator(height=img_height, width=img_width, channels=img_channels):\n","    # Input:\n","    inputs = ker.layers.Input(shape=[height, width, channels], name='input_image')\n","    inputs_patch = ker.layers.experimental.preprocessing.RandomCrop(height=75, width=75, name='input_image_patch')(inputs)\n","\n","    # Encoder:\n","    # 1 - (batch_size, 128, 128, 64)\n","    x = encoder_block(inputs_patch, 64,  4, 2, apply_instancenorm=False, activation=ker.layers.LeakyReLU(0.2), name='block_1')\n","    # 2 - (batch_size, 64, 64, 128)\n","    x = encoder_block(x, 128, 4, 2, apply_instancenorm=True, activation=ker.layers.LeakyReLU(0.2), name='block_2')\n","    # 3 - (batch_size, 32, 32, 256)\n","    x = encoder_block(x, 256, 4, 2, apply_instancenorm=True, activation=ker.layers.LeakyReLU(0.2), name='block_3')\n","    # 4 - (batch_size, 32, 32, 512)\n","    x = encoder_block(x, 512, 4, 1, apply_instancenorm=True, activation=ker.layers.LeakyReLU(0.2), name='block_4')\n","\n","    # Output:\n","    # 1 - (batch_size, 29, 29, 1)\n","    outputs = ker.layers.Conv2D(1, 4, strides=1, padding='valid', kernel_initializer=conv_initializer)(x)\n","\n","    discriminator = Model(inputs, outputs)\n","    return discriminator"],"metadata":{"id":"ikM_0sT-FrXq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Data and Augmentation\n","- The model was trained on 7036 \"normal\" images from the competition's dataset and an additional 1072 drawings by Monet from the following repository: https://efrosgans.eecs.berkeley.edu/cyclegan/datasets/monet2photo.zip."],"metadata":{"id":"I9d1RRY4i5Bu"}},{"cell_type":"code","source":["normal_photos = 'Add the path to the files here'\n","monet_photos = 'Add the path to the files here'"],"metadata":{"id":"gc4XnHpHnSmU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def monet_prep(image):\n","    image = tf.image.resize(image, size=[*(img_height, img_width)], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    image = tf.image.random_crop(image, size=[*(img_height, img_width, img_channels)])\n","    image = tf.cast(image, dtype=tf.float32)\n","    image = (image / 127.5) - 1.0\n","    return image\n","\n","def normal_prep(image):\n","    image = image.astype(np.float32) / 255.0\n","    image = (image - 0.5) * 2\n","    return image\n","\n","datagen_normal = ImageDataGenerator(\n","    horizontal_flip=True,\n","    preprocessing_function=normal_prep,\n",")\n","\n","datagen_monet = ImageDataGenerator(\n","    horizontal_flip=True,\n","    preprocessing_function=monet_prep,\n",")\n","\n","normal_image_generator = datagen_normal.flow_from_directory(\n","    normal_photos,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    shuffle=True\n",")\n","\n","monet_image_generator = datagen_monet.flow_from_directory(\n","    monet_photos,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='sparse',\n","    shuffle=False\n",")\n","\n","steps = ((monet_image_generator.samples)//batch_size)"],"metadata":{"id":"ClRBkoUiYwN5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["CycleGan Model\n","- The model definition, written using the Keras documentation: https://keras.io/examples/generative/cyclegan."],"metadata":{"id":"ZJMLijOuDTu3"}},{"cell_type":"code","source":["class CycleGan(Model):\n","    def __init__(\n","        self,\n","        monet_generator,\n","        normal_generator,\n","        monet_discriminator,\n","        normal_discriminator,\n","        lambda_cycle=10,\n","    ):\n","        super(CycleGan, self).__init__()\n","        self.monet_gen = monet_generator\n","        self.normal_gen = normal_generator\n","        self.monet_disc = monet_discriminator\n","        self.normal_disc = normal_discriminator\n","        self.lambda_cycle = lambda_cycle\n","\n","    # Override in Model\n","    def compile(\n","        self,\n","        monet_gen_optimizer,\n","        normal_gen_optimizer,\n","        monet_disc_optimizer,\n","        normal_disc_optimizer,\n","        gen_loss,\n","        disc_loss,\n","        cycle_loss, # monet -> normal -> monet || normal -> monet -> normal\n","        identity_loss\n","    ):\n","        super(CycleGan, self).compile()\n","        self.monet_gen_optimizer = monet_gen_optimizer\n","        self.normal_gen_optimizer = normal_gen_optimizer\n","        self.monet_disc_optimizer = monet_disc_optimizer\n","        self.normal_disc_optimizer = normal_disc_optimizer\n","        self.gen_loss = gen_loss\n","        self.disc_loss = disc_loss\n","        self.cycle_loss = cycle_loss\n","        self.identity_loss = identity_loss\n","\n","    # Override in Model\n","    def train_step(self, batch_data):\n","        real_monet, real_normal = batch_data\n","\n","        with tf.GradientTape(persistent=True) as tape:\n","            # Normal photo to monet and back to normal photo\n","            fake_monet = self.monet_gen(real_normal, training=True)\n","            cycled_normal = self.normal_gen(fake_monet, training=True)\n","\n","            # Monet to normal photo and back to Monet\n","            fake_normal = self.normal_gen(real_monet, training=True)\n","            cycled_monet = self.monet_gen(fake_normal, training=True)\n","\n","            # generating itself\n","            same_monet = self.monet_gen(real_monet, training=True)\n","            same_normal = self.normal_gen(real_normal, training=True)\n","\n","            # discriminator used to check, inputing real images\n","            disc_real_monet = self.monet_disc(real_monet, training=True)\n","            disc_real_normal = self.normal_disc(real_normal, training=True)\n","\n","            # discriminator used to check, inputing fake images\n","            disc_fake_monet = self.monet_disc(fake_monet, training=True)\n","            disc_fake_normal = self.normal_disc(fake_normal, training=True)\n","\n","            # evaluates generator loss\n","            monet_gen_loss = self.gen_loss(disc_fake_monet)\n","            normal_gen_loss = self.gen_loss(disc_fake_normal)\n","\n","            # evaluates total cycle consistency loss\n","            total_cycle_loss = self.cycle_loss(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss(real_normal, cycled_normal, self.lambda_cycle)\n","\n","            # evaluates total generator loss\n","            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss(real_monet, same_monet, self.lambda_cycle)\n","            total_normal_gen_loss = normal_gen_loss + total_cycle_loss + self.identity_loss(real_normal, same_normal, self.lambda_cycle)\n","\n","            # evaluates discriminator loss\n","            monet_disc_loss = self.disc_loss(disc_real_monet, disc_fake_monet)\n","            normal_disc_loss = self.disc_loss(disc_real_normal, disc_fake_normal)\n","\n","        # Calculate the gradients for generator and discriminator\n","        monet_generator_gradients = tape.gradient(total_monet_gen_loss, self.monet_gen.trainable_variables)\n","        self.monet_gen_optimizer.apply_gradients(zip(monet_generator_gradients, self.monet_gen.trainable_variables))\n","\n","        normal_generator_gradients = tape.gradient(total_normal_gen_loss, self.normal_gen.trainable_variables)\n","        self.normal_gen_optimizer.apply_gradients(zip(normal_generator_gradients, self.normal_gen.trainable_variables))\n","\n","        monet_discriminator_gradients = tape.gradient(monet_disc_loss, self.monet_disc.trainable_variables)\n","        self.monet_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients, self.monet_disc.trainable_variables))\n","\n","        normal_discriminator_gradients = tape.gradient(normal_disc_loss, self.normal_disc.trainable_variables)\n","        self.normal_disc_optimizer.apply_gradients(zip(normal_discriminator_gradients, self.normal_disc.trainable_variables))\n","\n","\n","        return {'monet_gen_loss': total_monet_gen_loss,\n","                'photo_gen_loss': total_normal_gen_loss,\n","                'monet_disc_loss': monet_disc_loss,\n","                'photo_disc_loss': normal_disc_loss\n","               }"],"metadata":{"id":"2Y1Wc9FLDSFu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Print Exampels\n","- This function prints 4 random images at each epoch."],"metadata":{"id":"dpwKnoN_Bmtn"}},{"cell_type":"code","source":["photos = [file for file in normal_photos if file.lower().endswith('.jpg')]\n","def select_random(folder_path, num_files=4):\n","    selected_files = rand.sample(photos, num_files)\n","    selected_files = [os.path.join(folder_path, file) for file in selected_files]\n","    return selected_files\n","\n","class GANMonitor(ker.callbacks.Callback):\n","\n","    def __init__(self, num_img=4, folder_path='path_to_folder'):\n","        self.num_img = num_img\n","        self.folder_path = folder_path\n","\n","    # Override in Callback\n","    def on_epoch_end(self, epoch, logs=None):\n","        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n","        image_paths = select_random(photos)\n","        for i, image_path in enumerate(image_paths):\n","            img = tf.io.read_file(image_path)\n","            img = tf.image.decode_jpeg(img, channels=3)\n","            img = tf.image.resize(img, [img_height, img_width])\n","            img = img / 127.5 - 1.0\n","\n","            prediction = self.model.monet_gen(tf.expand_dims(img, 0))[0].numpy()\n","            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n","            img = (img * 127.5 + 127.5).numpy().astype(np.uint8)\n","\n","            ax[i, 0].imshow(img)\n","            ax[i, 1].imshow(prediction)\n","            ax[i, 0].set_title(\"Input image\")\n","            ax[i, 1].set_title(\"Generated image\")\n","            ax[i, 0].axis(\"off\")\n","            ax[i, 1].axis(\"off\")\n","\n","            translated_image = ker.preprocessing.image.array_to_img(prediction)\n","            translated_image.save(f\"/content/TempImages/Generated11/generated_img_{i}_{epoch}.png\")\n","\n","        plt.show()\n","        plt.close()"],"metadata":{"id":"URRwe5nkDzJn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculating Loss Values\n","- **Discriminator Loss** (calc_discriminator_loss): The discriminator's goal is to distinguish between real and generated (fake) images. The discriminator loss consists of two parts:\n"," - **Real Loss**: Measures how well the discriminator is able to recognize real images. It compares the discriminator's predictions for real images to an array of ones (since real images should ideally yield a score close to 1).\n"," - **Generated Loss**: Assesses the discriminator's ability to identify generated images. It compares the discriminator's predictions for generated images to an array of zeros (since generated images should ideally yield a score close to 0).\n"," - **The total discriminator loss** is the average of these two losses, encouraging the discriminator to correctly classify real and generated images.\n","\n","- **Generator Loss** (calc_generator_loss): The generator's objective is to create images that are indistinguishable from real images. The generator loss measures how well the generated images fool the discriminator by comparing the discriminator's predictions for generated images to an array of ones. A lower generator loss indicates that the generator is producing more realistic images.\n","\n","- **Cycle Loss** (calc_cycle_loss): This loss is specific to CycleGANs or similar models where the goal includes not just one-way translation (e.g., normal to Monet) but also a reverse translation (e.g., Monet back to normal) to ensure consistency. The cycle loss measures the difference between an original image and a \"cycled\" image (an image that has been translated and then translated back). It encourages the preservation of content through the translation processes.\n","\n","- **Identity Loss** (calc_identity_loss): This loss is used to preserve color composition between the input and output images. When an image from the target domain is fed into the generator, the identity loss measures how much the generator changes this image. The idea is that if the generator is shown an image already in the target style, it should not change the image much."],"metadata":{"id":"15hHq6tKCCsU"}},{"cell_type":"code","source":["def calc_discriminator_loss(real, generated):\n","  real_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(real), real)\n","  generated_loss = losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n","  return (real_loss + generated_loss) * 0.5\n","\n","def calc_generator_loss(generated):\n","  return losses.BinaryCrossentropy(from_logits=True, reduction=losses.Reduction.NONE)(tf.ones_like(generated), generated)\n","\n","def calc_cycle_loss(real_image, cycled_image, Lambda):\n","  loss = tf.reduce_mean(tf.square(real_image - cycled_image)) # MSE/MeanSquaredError\n","  return Lambda * loss\n","\n","def calc_identity_loss(real_image, complex_image, Lambda):\n","  loss = tf.reduce_mean(tf.abs(real_image - complex_image)) # MAE/MeanAbsoluteError\n","  return Lambda * loss * 0.5"],"metadata":{"id":"s8xPctSpXdz-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implementing a CycleGAN for Monet Style Transfer and Photo Realism Restoration"],"metadata":{"id":"hjssO7QShhb3"}},{"cell_type":"code","source":["# monet_generator transforms normal photos to Monet-esque paintings:\n","monet_generator = generator(height=None, width=None, resnet_blocks=num_of_resnet_blocks)\n","# normal_generatort ransforms Monet paintings to be more like normal photos:\n","normal_generator = generator(height=None, width=None, resnet_blocks=num_of_resnet_blocks)\n","# monet_discriminator differentiates real Monet paintings and generated Monet paintings:\n","monet_discriminator = discriminator(height=None, width=None)\n","# normal_discriminator differentiates real photos and generated photos:\n","normal_discriminator = discriminator(height=None, width=None)\n","\n","monet_generator_optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_val)\n","normal_generator_optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_val)\n","monet_discriminator_optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_val)\n","normal_discriminator_optimizer = optimizers.Adam(learning_rate=learning_rate, beta_1=beta_val)\n","\n","gan_model = CycleGan(monet_generator, normal_generator, monet_discriminator, normal_discriminator)\n","gan_model.compile(monet_gen_optimizer=monet_generator_optimizer,\n","                  normal_gen_optimizer=normal_generator_optimizer,\n","                  monet_disc_optimizer=monet_discriminator_optimizer,\n","                  normal_disc_optimizer=normal_discriminator_optimizer,\n","                  gen_loss=calc_generator_loss,\n","                  disc_loss=calc_discriminator_loss,\n","                  cycle_loss=calc_cycle_loss,\n","                  identity_loss=calc_identity_loss\n","                  )"],"metadata":{"id":"72I8h_SpXkgV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reduce Linearly Learning Rate\n","- This function reduces the learning rate at every epoch starting from the defined epoch."],"metadata":{"id":"I_G3rVaIEfDk"}},{"cell_type":"code","source":["selected_epoch = 45\n","class LinearLRScheduler(tf.keras.callbacks.Callback):\n","    def __init__(self, optimizer, initial_lr, epoch_to_reduce):\n","        super(LinearLRScheduler, self).__init__()\n","        self.optimizer = optimizer\n","        self.initial_lr = initial_lr\n","        self.epoch_to_reduce = epoch_to_reduce\n","\n","    # Override in Callback\n","    def on_epoch_begin(self, epoch, logs=None):\n","        print(\"current lr is\",print(K.eval(self.optimizer.learning_rate)))\n","        if epoch >= self.epoch_to_reduce:\n","            new_lr = self.initial_lr * (1 - (epoch - self.epoch_to_reduce) / (self.params['epochs'] - self.epoch_to_reduce))\n","            tf.keras.backend.set_value(self.optimizer.learning_rate, new_lr)\n","            print(\"changed lr is\",print(K.eval(self.optimizer.learning_rate)))\n","linear_lr_scheduler_monet_gen = LinearLRScheduler(monet_generator_optimizer, initial_lr=learning_rate, epoch_to_reduce=selected_epoch)\n","linear_lr_scheduler_photo_gen = LinearLRScheduler(normal_generator_optimizer, initial_lr=learning_rate, epoch_to_reduce=selected_epoch)\n","linear_lr_scheduler_monet_disc = LinearLRScheduler(monet_discriminator_optimizer, initial_lr=learning_rate, epoch_to_reduce=selected_epoch)\n","linear_lr_scheduler_photo_disc = LinearLRScheduler(normal_discriminator_optimizer, initial_lr=learning_rate, epoch_to_reduce=selected_epoch)"],"metadata":{"id":"5qGCatyx3Gct"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Training of the model"],"metadata":{"id":"aVyY4H6QFH1j"}},{"cell_type":"code","source":["def get_from_directory(monet_generator, normal_generator, steps_per_epoch):\n","    while True:\n","        monet_images = monet_image_generator.next()[0]\n","        normal_images = normal_image_generator.next()[0]\n","        yield (monet_images, normal_images)\n","\n","plotter = GANMonitor()\n","checkpoint_filepath = \"/content/MonetModels11/cyclegan_checkpoints.{epoch:03d}\"\n","model_checkpoint_callback = ker.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True)\n","\n","gan_model.fit(\n","    get_from_directory(monet_image_generator, normal_image_generator, steps),\n","    epochs=epochs,\n","    steps_per_epoch=steps,\n","    callbacks=[plotter, linear_lr_scheduler_monet_gen, linear_lr_scheduler_photo_gen,\n","               linear_lr_scheduler_monet_disc, linear_lr_scheduler_photo_disc, model_checkpoint_callback],\n",")"],"metadata":{"id":"xIJo8SahXoc7"},"execution_count":null,"outputs":[]}]}